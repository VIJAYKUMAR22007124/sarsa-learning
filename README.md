# SARSA Learning Algorithm


## AIM
To develop a Python program to find the optimal policy for the given RL environment using SARSA-Learning and compare the state values with the Monte Carlo method.

## PROBLEM STATEMENT
A Python program to find the optimal policy for a reinforcement learning environment using the SARSA-Learning algorithm. The program will also implement the Monte Carlo method to estimate state values. The goal is to compare the state values generated by both approaches to analyze their efficiency. The environment will be defined, and performance metrics will be used to evaluate the results.

## SARSA LEARNING ALGORITHM


1. **Initialization**: Set up the environment variables like state space (`nS`), action space (`nA`), the Q-table (`Q`), and tracking arrays for policies and Q-values over episodes.

2. **Action Selection**: Define an epsilon-greedy policy to select actions, balancing exploration (random actions) and exploitation (best known actions) using a decaying epsilon value.

3. **Learning Rate & Epsilon Decay**: Use decaying schedules for the learning rate (`alphas`) and exploration rate (`epsilons`) over episodes for better convergence.

4. **Episode Loop**: For each episode, run a loop that:
   - Selects an action based on the current policy.
   - Takes a step in the environment, observes the next state, reward, and updates the Q-table using the SARSA update rule.

5. **Track Results**: After each episode, store the updated Q-values and policies to analyze the learning process, and compute the state-value function `V` from the optimal Q-values at the end.

## SARSA LEARNING FUNCTION
### Name: B VIJAY KUMAR
### Register Number: 212222230173

```
def sarsa(env,
          gamma=1.0,
          init_alpha=0.5,
          min_alpha=0.01,
          alpha_decay_ratio=0.5,
          init_epsilon=1.0,
          min_epsilon=0.1,
          epsilon_decay_ratio=0.9,
          n_episodes=3000):

    nS, nA = env.observation_space.n, env.action_space.n
    pi_track = []
    Q = np.zeros((nS, nA), dtype=np.float64)
    Q_track = np.zeros((n_episodes, nS, nA), dtype=np.float64)

    select_action = lambda state, Q, epsilon: np.argmax(Q[state]) if np.random.random() > epsilon else np.random.randint(len(Q[state]))

    alphas = decay_schedule(init_alpha, min_alpha, alpha_decay_ratio, n_episodes)
    epsilons = decay_schedule(init_epsilon, min_epsilon, epsilon_decay_ratio, n_episodes)

    for e in tqdm(range(n_episodes), leave=False):
        state, done = env.reset(), False
        action = select_action(state, Q, epsilons[e])

        while not done:
            next_state, reward, done, _ = env.step(action)
            next_action = select_action(next_state, Q, epsilons[e])

            td_target = reward + gamma * Q[next_state, np.argmax(Q[next_state])] * (1 - done)
            td_error = td_target - Q[state, action]
            Q[state, action] += alphas[e] * td_error

            state = next_state
            action = next_action

        Q_track[e] = Q.copy()
        pi_track.append(np.argmax(Q, axis=1))

    V = np.max(Q, axis=1)
    pi = lambda s: np.argmax(Q[s])

    return Q, V, pi, Q_track, pi_track

```

## OUTPUT:

![image](https://github.com/user-attachments/assets/ff1012c4-75f3-4668-a6b2-218e3cd1fc34)

### FVMC

![image](https://github.com/user-attachments/assets/6cde0984-d72b-4a65-b9f3-927931509052)

### SARSA
![image](https://github.com/user-attachments/assets/0a71def6-66e4-4175-9fbc-2a733675d32a)


## RESULT:

Thus , the program to find the optimal policy for the given RL environment using SARSA-Learning and comparing the state values with the Monte Carlo method is sucessfully developed 
